title : Getting Started with Python description : In this chapter we will go trough the essential steps that you will need to take
before beginning to build predictive models.
--- type:NormalExercise lang:python xp:100 skills:2 key:49f71e27bd

How it works
Welcome to our Kaggle Machine Learning Tutorial. In this tutorial, you will explore how to tackle Kaggle Titanic competition
using Python and Machine Learning. In case you're new to Python, it's recommended that you first take our free Introduction to
Python for Data Science Tutorial. Furthermore, while not required, familiarity with machine learning techniques is a plus so you
can get the maximum out of this tutorial.

In the editor on the right, you should type Python code to solve the exercises. When you hit the 'Submit Answer' button, every
line of code is interpreted and executed by Python and you get a message whether or not your code was correct. The output of
your Python code is shown in the console in the lower right corner. Python makes use of the # sign to add comments; these
lines are not run as Python code, so they will not influence your result.
You can also execute Python commands straight in the console. This is a good way to experiment with Python code, as your
submission is not checked for correctness.
*** =instructions
In the editor to the right, you see some Python code and annotations. This is what a typical exercise will look like.
To complete the exercise and see how the interactive environment works add the code to compute y and hit the Submit

Answer button. Don't forget to print the result.
		*** =hint
		Just add a line of Python code that calculates the product of 6 and 9, just like the example in the sample code!
		*** =pre_exercise_code
		# no pre_exercise_code
		*** =sample_code
		#Compute x = 4 * 3 and print the result
		x = 4 * 3
		print(x)
		#Compute y = 6 * 9 and print the result
		y = 6*9
		print(y)


Get the Data with Pandas
When the Titanic sank, 1502 of the 2224 passengers and crew were killed. One of the main reasons for this high level of
casualties was the lack of lifeboats on this self-proclaimed "unsinkable" ship.
Those that have seen the movie know that some individuals were more likely to survive the sinking (lucky Rose) than others
(poor Jack). In this course, you will learn how to apply machine learning techniques to predict a passenger's chance of surviving
using Python.
Let's start with loading in the training and testing set into your Python environment. You will use the training set to build your
model, and the test set to validate it. The data is stored on the web as csv files; their URLs are already available as character
strings in the sample code. You can load this data with the read_csv() method from the Pandas library.
*** =instructions
First, import the Pandas library as pd.
Load the test data similarly to how the train data is loaded.
Inspect the first couple rows of the loaded dataframes using the .head() method with the code provided.
*** =hint
You can load in the training set with train = pd.read_csv(train_url)
To print a variable to the console, use the print function on a new line.


		*** =solution
		# Import the Pandas library
		import pandas as pd
		# Load the train and test datasets to create two DataFrames
		train_url = "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv"
		train = pd.read_csv(train_url)
		test_url = "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv"
		test = pd.read_csv(test_url)
		#Print the `head` of the train and test dataframes
		print(train.head())
		print(test.head())

Understanding your data
Before starting with the actual analysis, it's important to understand the structure of your data. Both test and train are
DataFrame objects, the way pandas represent datasets. You can easily explore a DataFrame using the .describe() method.
.describe() summarizes the columns/features of the DataFrame, including the count of observations, mean, max and so on.

Another useful trick is to look at the dimensions of the DataFrame. This is done by requesting the .shape attribute of your
DataFrame object. (ex. your_data.shape )
The training and test set are already available in the workspace, as train and test . Apply .describe() method and print the
.shape attribute of the training set. Which of the following statements is correct?
*** =instructions
The training set has 891 observations and 12 variables, count for Age is 714.
The training set has 418 observations and 11 variables, count for Age is 891.
The testing set has 891 observations and 11 variables, count for Age is 891.
The testing set has 418 observations and 12 variables, count for Age is 714.
*** =hint To see the description of the test variable try test.describe() .

		solution:

		either of:
		print(train.describe)
		or
		train.describe



Rose vs Jack, or Female vs Male
100xp
How many people in your training set survived the disaster with the Titanic? To see this, you can use the value_counts() method in combination with standard bracket notation to select a single column of a DataFrame:

# absolute numbers
train["Survived"].value_counts()

# percentages
train["Survived"].value_counts(normalize = True)
If you run these commands in the console, you'll see that 549 individuals died (62%) and 342 survived (38%). A simple way to predict heuristically could be: "majority wins". This would mean that you will predict every unseen observation to not survive.

To dive in a little deeper we can perform similar counts and percentage calculations on subsets of the Survived column. For example, maybe gender could play a role as well? You can explore this using the .value_counts() method for a two-way comparison on the number of males and females that survived, with this syntax:

train["Survived"][train["Sex"] == 'male'].value_counts()
train["Survived"][train["Sex"] == 'female'].value_counts()
To get proportions, you can again pass in the argument normalize = True to the .value_counts() method.

Instructions
Calculate and print the survival rates in absolute numbers using values_counts() method.
Calculate and print the survival rates as proportions by setting the normalize argument to True.
Repeat the same calculations but on subsets of survivals based on Sex.
Take Hint (-30xp)

		# Passengers that survived vs passengers that passed away
		print(train["Survived"].value_counts())

		# As proportions
		print(train["Survived"].value_counts(normalize = True))

		# Males that survived vs males that passed away
		print(train["Survived"][train["Sex"] == 'male'].value_counts())

		# Females that survived vs Females that passed away
		print(train["Survived"][train["Sex"] == 'female'].value_counts())

		# Normalized male survival
		print(train["Survived"][train["Sex"] == 'male'].value_counts(normalize = True))

		# Normalized female survival
		print(train["Survived"][train["Sex"] == 'female'].value_counts(normalize = True))

Does age play a role?
100xp
Another variable that could influence survival is age; since it's probable that children were saved first. You can test this by creating a new column with a categorical variable Child. Child will take the value 1 in cases where age is less than 18, and a value of 0 in cases where age is greater than or equal to 18.

To add this new variable you need to do two things (i) create a new column, and (ii) provide the values for each observation (i.e., row) based on the age of the passenger.

Adding a new column with Pandas in Python is easy and can be done via the following syntax:

your_data["new_var"] = 0
This code would create a new column in the train DataFrame titled new_var with 0 for each observation.

To set the values based on the age of the passenger, you make use of a boolean test inside the square bracket operator. With the []-operator you create a subset of rows and assign a value to a certain variable of that subset of observations. For example,

train["new_var"][train["Fare"] > 10] = 1
would give a value of 1 to the variable new_var for the subset of passengers whose fares greater than 10. Remember that new_var has a value of 0 for all other values (including missing values).

A new column called Child in the train data frame has been created for you that takes the value NaN for all observations.

Instructions
Set the values of Child to 1 is the passenger's age is less than 18 years.
Then assign the value 0 to observations where the passenger is greater than or equal to 18 years in the new Child column.
Compare the normalized survival rates for those who are <18 and those who are older. Use code similar to what you had in the previous exercise.
Take Hint (-30xp)

		# Create the column Child and assign to 'NaN'
		train["Child"] = float('NaN')

		# Assign 1 to passengers under 18, 0 to those 18 or older. Print the new column.
		train["Child"][train["Age"] < 18] = 1
		train["Child"][train["Age"] >= 18] = 0
		print(train["Child"])
		# Print normalized Survival Rates for passengers under 18
		print(train["Survived"][train["Child"] == 1].value_counts(normalize = True))

		# Print normalized Survival Rates for passengers 18 or older
		print(train["Survived"][train["Child"] == 0].value_counts(normalize = True))

First Prediction
100xp
In one of the previous exercises you discovered that in your training set, females had over a 50% chance of surviving and males had less than a 50% chance of surviving. Hence, you could use this information for your first prediction: all females in the test set survive and all males in the test set die.

You use your test set for validating your predictions. You might have seen that contrary to the training set, the test set has no Survived column. You add such a column using your predicted values. Next, when uploading your results, Kaggle will use this variable (= your predictions) to score your performance.

Instructions
Create a variable test_one, identical to dataset test
Add an additional column, Survived, that you initialize to zero.
Use vector subsetting like in the previous exercise to set the value of Survived to 1 for observations whose Sex equals "female".
Print the Survived column of predictions from the test_one dataset.
Take Hint (-30xp)

		# Create a copy of test: test_one
		test_one = test

		# Initialize a Survived column to 0
		test_one["Survived"] = 0

		# Set Survived to 1 if Sex equals "female" and print the `Survived` column from `test_one`
		test_one["Survived"][test_one["Sex"] == "female"] = 1
		test_one["Survived"][test_one["Sex"] == "male"] = 0
		print(test_one["Survived"])
		
		
Intro to decision trees
100xp
In the previous chapter, you did all the slicing and dicing yourself to find subsets that have a higher chance of surviving. A decision tree automates this process for you and outputs a classification model or classifier.

Conceptually, the decision tree algorithm starts with all the data at the root node and scans all the variables for the best one to split on. Once a variable is chosen, you do the split and go down one level (or one node) and repeat. The final nodes at the bottom of the decision tree are known as terminal nodes, and the majority vote of the observations in that node determine how to predict for new observations that end up in that terminal node.

First, let's import the necessary libraries:

Instructions
Import the numpy library as np
From sklearn import the tree
Take Hint (-30xp)

		# Import the Numpy library
		import numpy as np
		# Import 'tree' from scikit-learn library
		from sklearn import tree

Cleaning and Formatting your Data
100xp
Before you can begin constructing your trees you need to get your hands dirty and clean the data so that you can use all the features available to you. In the first chapter, we saw that the Age variable had some missing value. Missingness is a whole subject with and in itself, but we will use a simple imputation technique where we substitute each missing value with the median of the all present values.

train["Age"] = train["Age"].fillna(train["Age"].median())
Another problem is that the Sex and Embarked variables are categorical but in a non-numeric format. Thus, we will need to assign each class a unique integer so that Python can handle the information. Embarked also has some missing values which you should impute witht the most common class of embarkation, which is "S".

Instructions
Assign the integer 1 to all females
Impute missing values in Embarked with class S. Use .fillna() method.
Replace each class of Embarked with a uniques integer. 0 for S, 1 for C, and 2 for Q.
Print the Sex and Embarked columns
Take Hint (-30xp)

		# Convert the male and female groups to integer form
		train["Sex"][train["Sex"] == "male"] = 0
		train["Sex"][train["Sex"] == "female"] = 1
		# Impute the Embarked variable
		train["Embarked"] = train["Embarked"].fillna("S")

		# Convert the Embarked classes to integer form
		train["Embarked"][train["Embarked"] == "S"] = 0
		train["Embarked"][train["Embarked"] == "C"] = 1
		train["Embarked"][train["Embarked"] == "Q"] = 2

		#Print the Sex and Embarked columns
		print(train["Sex"])
		print(train["Embarked"])

Creating your first decision tree
100xp
You will use the scikit-learn and numpy libraries to build your first decision tree. scikit-learn can be used to create tree objects from the DecisionTreeClassifier class. The methods that we will use take numpy arrays as inputs and therefore we will need to create those from the DataFrame that we already have. We will need the following to build a decision tree

target: A one-dimensional numpy array containing the target/response from the train data. (Survival in your case)
features: A multidimensional numpy array containing the features/predictors from the train data. (ex. Sex, Age)
Take a look at the sample code below to see what this would look like:

target = train["Survived"].values

features = train[["Sex", "Age"]].values

my_tree = tree.DecisionTreeClassifier()

my_tree = my_tree.fit(features, target)

One way to quickly see the result of your decision tree is to see the importance of the features that are included. This is done by requesting the .feature_importances_ attribute of your tree object. Another quick metric is the mean accuracy that you can compute using the .score() function with features_one and target as arguments.

Ok, time for you to build your first decision tree in Python! The train and testing data from chapter 1 are available in your workspace.

Instructions
Build the target and features_one numpy arrays. The target will be based on the Survived column in train. The features array will be based on the variables Passenger, Class, Sex, Age, and Passenger Fare
Build a decision tree my_tree_one to predict survival using features_one and target
Look at the importance of features in your tree and compute the score
Take Hint (-30xp)

		# Print the train data to see the available features
		print (train)

		# Create the target and features numpy arrays: target, features_one
		target = train["Survived"].values
		features_one = train[["Pclass", "Sex", "Age", "Fare"]].values

		# Fit your first decision tree: my_tree_one
		my_tree_one = tree.DecisionTreeClassifier()
		my_tree_one = my_tree_one.fit(features_one, target)

		# Look at the importance and score of the included features
		print(my_tree_one.feature_importances_)
		print(my_tree_one.score(features_one, target))

Interpreting your decision tree
50xp
The feature_importances_ attribute make it simple to interpret the significance of the predictors you include. Based on your decision tree, what variable plays the most important role in determining whether or not a passenger survived? Your model (my_tree_one) is available in the console.

Possible Answers
Click or Press Ctrl+1 to focus
Passenger Class
Sex/Gender
Passenger Fare
Age
Take Hint (-15xp)

Interpreting your decision tree
50xp
The feature_importances_ attribute make it simple to interpret the significance of the predictors you include. Based on your decision tree, what variable plays the most important role in determining whether or not a passenger survived? Your model (my_tree_one) is available in the console.

Possible Answers
Click or Press Ctrl+1 to focus
Passenger Class
Sex/Gender
Passenger Fare
Age
Take Hint (-15xp)

		In [1]: print (my_tree_one.feature_importances_)
		[ 0.1269655   0.31274009  0.23147703  0.32881738]

		Passenger Fare
		
Predict and submit to Kaggle
100xp
To send a submission to Kaggle you need to predict the survival rates for the observations in the test set. In the last exercise of the previous chapter, we created simple predictions based on a single subset. Luckily, with our decision tree, we can make use of some simple functions to "generate" our answer without having to manually perform subsetting.

First, you make use of the .predict() method. You provide it the model (my_tree_one), the values of features from the dataset for which predictions need to be made (test). To extract the features we will need to create a numpy array in the same way as we did when training the model. However, we need to take care of a small but important problem first. There is a missing value in the Fare feature that needs to be imputed.

Next, you need to make sure your output is in line with the submission requirements of Kaggle: a csv file with exactly 418 entries and two columns: PassengerId and Survived. Then use the code provided to make a new data frame using DataFrame(), and create a csv file using to_csv() method from Pandas.

Instructions
Impute the missing value for Fare in row 153 with the median of the column.
Make a prediction on the test set using the .predict() method and my_tree_one. Assign the result to my_prediction.
Create a data frame my_solution containing the solution and the passenger ids from the test set. Make sure the solution is in line with the standards set forth by Kaggle by naming the column appropriately.
Take Hint (-30xp)

		# Impute the missing value with the median
		test.Fare[152] = test.Fare.median()

		# Extract the features from the test set: Pclass, Sex, Age, and Fare.
		test_features = test[["Pclass", "Sex", "Age", "Fare"]].values

		# Make your prediction using the test set
		my_prediction = my_tree_one.predict(test_features)
		print(my_prediction)
		# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions
		PassengerId =np.array(test["PassengerId"]).astype(int)
		my_solution = pd.DataFrame(my_prediction, PassengerId, columns = ["Survived"])
		print(my_solution)

		# Check that your data frame has 418 entries
		print(my_solution.shape)

		# Write your solution to a csv file with the name my_solution.csv
		my_solution.to_csv("my_solution_one.csv", index_label = ["PassengerId"])


Overfitting and how to control it
100xp
When you created your first decision tree the default arguments for max_depth and min_samples_split were set to None. This means that no limit on the depth of your tree was set. That's a good thing right? Not so fast. We are likely overfitting. This means that while your model describes the training data extremely well, it doesn't generalize to new data, which is frankly the point of prediction. Just look at the Kaggle submission results for the simple model based on Gender and the complex decision tree. Which one does better?

Maybe we can improve the overfit model by making a less complex model? In DecisionTreeRegressor, the depth of our model is defined by two parameters: - the max_depth parameter determines when the splitting up of the decision tree stops. - the min_samples_split parameter monitors the amount of observations in a bucket. If a certain threshold is not reached (e.g minimum 10 passengers) no further splitting can be done.

By limiting the complexity of your decision tree you will increase its generality and thus its usefulness for prediction!

Instructions
Include the Siblings/Spouses Aboard, Parents/Children Aboard, and Embarked features in a new set of features.
Fit your second tree my_tree_two with the new features, and control for the model compelexity by toggling the max_depth and min_samples_split arguments.
Take Hint (-30xp)

		# Create a new array with the added features: features_two
		features_two = train[["Pclass","Age","Sex","Fare", "SibSp", "Parch", "Embarked"]].values

		#Control overfitting by setting "max_depth" to 10 and "min_samples_split" to 5 : my_tree_two
		max_depth = 10
		min_samples_split = 5
		my_tree_two = tree.DecisionTreeClassifier(max_depth = 10, min_samples_split = 5, random_state = 1)
		my_tree_two = my_tree_two.fit(features_two, target)

		#Print the score of the new decison tree
		print(my_tree_two.score(features_two, target))


Feature-engineering for our Titanic data set
100xp
Data Science is an art that benefits from a human element. Enter feature engineering: creatively engineering your own features by combining the different existing variables.

While feature engineering is a discipline in itself, too broad to be covered here in detail, you will have a look at a simple example by creating your own new predictive attribute: family_size.

A valid assumption is that larger families need more time to get together on a sinking ship, and hence have lower probability of surviving. Family size is determined by the variables SibSp and Parch, which indicate the number of family members a certain passenger is traveling with. So when doing feature engineering, you add a new variable family_size, which is the sum of SibSp and Parch plus one (the observation itself), to the test and train set.

Instructions
Create a new train set train_two that differs from train only by having an extra column with your feature engineered variable family_size.
Add your feature engineered variable family_size in addition to Pclass, Sex, Age, Fare, SibSp and Parch to features_three.
Create a new decision tree as my_tree_three and fit the decision tree with your new feature set features_three. Then check out the score of the decision tree.
Take Hint (-30xp)

		# Create train_two with the newly defined feature
		train_two = train.copy()
		train_two["family_size"] = train["SibSp"] + train["Parch"] + 1

		# Create a new feature set and add the new feature
		features_three = train_two[["Pclass", "Sex", "Age", "Fare", "SibSp", "Parch", "family_size"]].values

		# Define the tree classifier, then fit the model
		my_tree_three = tree.DecisionTreeClassifier()
		my_tree_three = my_tree_three.fit(features_three, target)

		# Print the score of this decision tree
		print(my_tree_three.score(features_three, target))

A Random Forest analysis in Python
100xp
A detailed study of Random Forests would take this tutorial a bit too far. However, since it's an often used machine learning technique, gaining a general understanding in Python won't hurt.

In layman's terms, the Random Forest technique handles the overfitting problem you faced with decision trees. It grows multiple (very deep) classification trees using the training set. At the time of prediction, each tree is used to come up with a prediction and every outcome is counted as a vote. For example, if you have trained 3 trees with 2 saying a passenger in the test set will survive and 1 says he will not, the passenger will be classified as a survivor. This approach of overtraining trees, but having the majority's vote count as the actual classification decision, avoids overfitting.

Building a random forest in Python looks almost the same as building a decision tree; so we can jump right to it. There are two key differences, however. Firstly, a different class is used. And second, a new argument is necessary. Also, we need to import the necessary library from scikit-learn.

Use RandomForestClassifier() class instead of the DecisionTreeClassifier() class.
n_estimators needs to be set when using the RandomForestClassifier() class. This argument allows you to set the number of trees you wish to plant and average over.
The latest training and testing data are preloaded for you.

Instructions
Build the random forest with n_estimators set to 100.
Fit your random forest model with inputs features_forest and target.
Compute the classifier predictions on the selected test set features.
Take Hint (-30xp)

		# Import the `RandomForestClassifier`
		from sklearn.ensemble import RandomForestClassifier

		# We want the Pclass, Age, Sex, Fare,SibSp, Parch, and Embarked variables
		features_forest = train[["Pclass", "Age", "Sex", "Fare", "SibSp", "Parch", "Embarked"]].values

		# Building and fitting my_forest
		forest = RandomForestClassifier(max_depth = 10, min_samples_split=2, n_estimators = 100, random_state = 1)
		my_forest = forest.fit(features_forest, target)

		# Print the score of the fitted random forest
		print(my_forest.score(features_forest, target))

		# Compute predictions on our test set features then print the length of the prediction vector
		test_features = test[["Pclass", "Age", "Sex", "Fare", "SibSp", "Parch", "Embarked"]].values
		pred_forest = my_forest.predict(test_features)
		print(len(pred_forest))

Interpreting and Comparing
100xp
Remember how we looked at .feature_importances_ attribute for the decision trees? Well, you can request the same attribute from your random forest as well and interpret the relevance of the included variables. You might also want to compare the models in some quick and easy way. For this, we can use the .score() method. The .score() method takes the features data and the target vector and computes mean accuracy of your model. You can apply this method to both the forest and individual trees. Remember, this measure should be high but not extreme because that would be a sign of overfitting.

For this exercise, you have my_forest and my_tree_two available to you. The features and target arrays are also ready for use.

Instructions
Explore the feature importance for both models
Compare the mean accuracy score of the two models
Take Hint (-30xp)

		#Request and print the `.feature_importances_` attribute
		print(my_tree_two.feature_importances_)
		print(my_forest.feature_importances_)

		#Compute and print the mean accuracy score for both models
		print(my_tree_two.score(features_two, target))
		print(my_forest.score(features_forest, target))



Conclude and Submit
50xp
Based on your finding in the previous exercise determine which feature was of most importance, and for which model. After this final exercise, you will be able to submit your random forest model to Kaggle! Use my_forest, my_tree_two, and feature_importances_ to answer the question.

Possible Answers
Click or Press Ctrl+1 to focus
The most important feature was "Age", but it was more significant for "my_tree_two"
The most important feature was "Sex", but it was more significant for "my_tree_two"
The most important feature was "Sex", but it was more significant for "my_forest"
The most important feature was "Age", but it was more significant for "my_forest"
Take Hint (-15xp)

The most important feature was "Sex", but it was more significant for "my_tree_two"


